- Notes from reading the 2021 paper
  + Title: Why is AI hard and Physics simple?
  + Author: Daniel A. Roberts
  + Published: online [2021-03-31 Wed]
    * https://export.arxiv.org/abs/2104.00008v1
- This 29 page preprint serves as an appetizer to the 449 page book "[[https://deeplearningtheory.com/PDLT.pdf][Principles of Deep Learning Theory]]" that came out as pdf in June 2021
- Understanding how AI works is inextricably entangled with understanding how the human mind works---how we perceive and think
- *Sparsity* is introduced as a key principle in physics: The number of parameters needed to define a model should be far less than the number of experiments that the theory describes
- *Spatial locality* is another important principle in physics---only neighbors can interact, and in some sense the concept of neighbor and space as such is defined precisely through the spatial locality of interactions
- Cognitive scientists take a top-down approach to ML by trying to understand it through human learning
- Physics tends to operate in a bottom-up approach and this may also be useful in modeling ML, even if applying it to understanding humans will lead nowhere
- Compared to quantum field theory, DL is a classical statistical theory  and not quantum, also its statistical variables are not forming a field---nevertheless, both are/can be approached as *effective theories*, i.e. they describe  observations without making claims about modeling the actual causes of observed effects
- Knowing the architecture and value of all weights and biases does not give us any useful insight into how the function (the network) would operate on some input
  + Given all the parameters and some input $x$, the easiest way to say something about $f(x)$ would be to run $x$ through the network
- In the infinite-width limit (thermodynamic limit or large-$N$ limit), but keeping the depth $L$ fixed
  + All neural interactions turn off
  + All neural distributions converge on multivariate Gaussians
  + Correlations are completely determined by the pairwise covariance matrix (likely that same statement as distributions being Gaussians, given that all moments higher than the second vanish for these)
  + Intermediate representations are fixed from the start and thus independent of training data
  + The network behaves as if it isn't deep and it also doesn't learn
  + The limit is too simple!
- Taking another page out of the book of theoretical physics, we take a look at *perturbation theory*
  + We either have an infinitesimally small, dimensionless, parameter $\epsilon$ or we turn an infinitely big one $N$, into a small one by defining $\epsilon=1/N$ and Taylor expanding in that (this is called an $1/N$ expansion)
- The first correction to the infinite $N$ limit that we now expand around is $O(1/N)$ and introduces correlations between collections of four neurons
  + The, now finite width, output distributions are no longer multivariate Gaussians but have higher order moments
  + Further corrections, in higher orders of $1/N$ also leads to longer-range correlations
  + These perturbative models span the spectrum from sparse to complex
- The final tool introduced from physics is *renormalization group flow* as Ken Wilson introduced it in 1971
  + The "flow" is generated by repeated marginalizations (coarse-graining/down-sampling) of the system, resulting in an effective theory of coarse-grained observables
  + Interactions that grow with the flow are called /relevant/, those that decrease in strength are called /irrelevant/
  + Renaming the DL version *representation group flow* and applying it to the lowest order expansion in $1/N$ reveals that the lowest order corrections scale linearly with $L$
  + The proper perturbative parameter for the toy models is thus the aspect ratio $L/N$ of depth to width of the network
- The scales $L$ and $N$ are supposedly responsible for the wide range of hyperparameters encountered in the wild (see their book)
- Most networks will show exponential growth or decay of signals as we iterate over (train) the weights 
  + Only networks whose initial distribution of weights and biases are carefully tuned to a *critical point*  will not have exponential decay or growth but reach *criticality* and show self-similar behavior
- At the critical point we should expect system-wide fluctuations (actually, typically a power-law distribution of fluctuations, so all sizes are possible)
  + Fluctuations happen as a function of different instantiations, i.e. different realizations of initial conditions can lead to very different results
  + Analysis shows that these fluctuations scale with the emergent cutoff of $~L/N$
  + We therefore want $L/N < 1$ to suppress fluctuations
  + But, we also want $L/N > 0$ to get any learning at all (recall the first naive thermodynamic limit in $N$)
- In summary, this paper is a preview of a new book and has the goal of finding a bottom-up formalism that allows (certain types of) physicists to understand DL by taking an /effective theory/ approach
  + Apart from a deeper understanding of DL and the joy of encountering old friends from theoretical physics, the perturbative analysis also promises some practical advantages in guiding the practitioner in their choice of architecture, initial conditions, and hyperparameters---hopefully the book will contain more concrete information on this point
  + Also, this interpretation explains how over-parameterized (more tunable parameters than training data) networks are "secretly" quite simple and sparse in their /data-dependent coupling/ description 
