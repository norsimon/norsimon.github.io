<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-10-06 Wed 14:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Simon F. Nørrelykke" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5d84f62">1. What is this post about?</a></li>
<li><a href="#org4703fd8">2. Short take</a></li>
<li><a href="#org7b96538">3. What do they do?</a></li>
<li><a href="#org7cecb95">4. How do they do it?</a></li>
<li><a href="#orgabdb2e1">5. Why do they do it?</a></li>
<li><a href="#orgc2c51af">6. Conclusions and open questions</a></li>
</ul>
</div>
</div>
<div id="outline-container-org5d84f62" class="outline-2">
<h2 id="org5d84f62"><span class="section-number-2">1.</span> What is this post about?</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Notes from reading the 2021 paper:
<ul class="org-ul">
<li>"Mining for strong gravitational lenses with self-supervised learning"</li>
<li>by George Stein, Jacqueline Blaum, Peter Harrington, Tomislav Medan, and Zarija Lukic</li>
<li><a href="https://arxiv.org/abs/2110.00023">https://arxiv.org/abs/2110.00023</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4703fd8" class="outline-2">
<h2 id="org4703fd8"><span class="section-number-2">2.</span> Short take</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>A <b>self-supervised convolutional neural networks</b> (CNN) is trained to identify candidates for strong gravitational lensing in already published image data sets.</li>
<li>The self-supervised approach addresses shortcomings in supervised CNNs that depend on larger amounts of (well-balanced classes of) labeled data for training.</li>
<li>The underlying idea is to train the CNN to "build meaningful embeddings or representations by solving contrived tasks during training", i.e. the network is learning which features in an image it should pay attention to.</li>
<li>Compared to auto-encoders, self-supervised CNNs produce better representations that generalize to different tasks.
<ul class="org-ul">
<li>The representations learned are also semantically meaningful</li>
</ul></li>
<li>A web-based tool for similarity searches is made available at <a href="https://github.com/georgestein/ssl-legacysurvey">https://github.com/georgestein/ssl-legacysurvey</a> (link there to <a href="https://share.streamlit.io/georgestein/galaxy_search">https://share.streamlit.io/georgestein/galaxy_search</a>)</li>
<li>The learned representation is task-agnostic, in the sense that no lens label was used, though the data-augmentations were obviously designed to learn to ignore aspects of the images that are not relevant to detection of lensing.</li>
<li>The actual classification of lens-candidates is done via e.g. a linear classifier, taking as input the feature vectors from running the labeled lens images and the unlabeled images through the pre-trained CNN.
<ul class="org-ul">
<li>So, ultimately and obviously, some labeled data is still needed for the detection of candidates.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7b96538" class="outline-2">
<h2 id="org7b96538"><span class="section-number-2">3.</span> What do they do?</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>They train a self-supervised CNN on a large volume of unlabeled data, so that it sees many different realizations of each morphology and all possible (observed) morphologies.
<ul class="org-ul">
<li>This is where the CNN learns the relevant representations of the images, i.e., which features to focus on.</li>
</ul></li>
<li>Then, they use the learned representations to identify new candidates, by doing similarity searches and classification.</li>
<li>This way, they identify 1'192 new strong-lens candidates that were also visually inspected.</li>
</ul>
</div>
</div>

<div id="outline-container-org7cecb95" class="outline-2">
<h2 id="org7cecb95"><span class="section-number-2">4.</span> How do they do it?</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>The training is done by using a <b>contrastive loss function</b> to make the network learn to distinguish between pairs of different images and pairs of similar images, the latter being created by augmentations (rotation, scaling, noise, etc) 
<ul class="org-ul">
<li>These augmentations must reflect the inherent symmetries, i.e. what is <i>not</i> important in the dataset, so that the network learns what to ignore, e.g. noise, rotation, scaling, translation, point-spread-function and other observational artifacts.</li>
<li>The learned representations are thus, in a sense, the invariant properties of the images (objects imaged)</li>
</ul></li>
<li>A subset of just 3.5 million images is used for training a <b>ResNet50 network</b> with queue length \(K=262'144\)
<ul class="org-ul">
<li>8 hours on 8 NVIDIA Tesla V100 GPUs</li>
</ul></li>
<li>All 76 million images were then pushed through the trained network to distill out a representation vector \(z\), of length 2'048 for each image (training images are \(96 \times 96=9'216\) pixels large)
<ul class="org-ul">
<li>This representation vector is largely independent of the symmetries used to generate the augmented data-set (noise, scale, etc.)</li>
</ul></li>
<li>Lens candidates were identified by three different methods
<ol class="org-ol">
<li>Similarity search, by first calculating the <b>cosine similarity</b> (normalized scalar product) in representation space, i.e. the angle between two representation vectors \(z_i\) and \(z_j\), 
<ul class="org-ul">
<li>For each of the millions of images they identified the 999 most similar to each, thus similar images can be grouped together</li>
<li>Similarity search could then be performed based on a single labeled image.</li>
<li>This approach is also good for improving the training set, by using it to identify examples of false positives</li>
</ul></li>
<li>Supervised training of a linear classifier, to predict the probability that at given input image is a lens
<ul class="org-ul">
<li>A matter of binary partitioning of the 2'048-dimensional representation space into lens and non-lens.</li>
<li>The linear classifier was of the form \(y=W^Tz+b\) (\(y\): lens-probability, \(W\): 2'048-dimensional weight matrix to learn, \(z\): representation vector, \(b\): bias to learn)</li>
<li>Linear classifier implemented in Pytorch using a <b>binary cross entropy loss</b> and trained in minutes on a single CPU.</li>
<li>Non-linear random-forest approach led to overfitting and worse performance.</li>
</ul></li>
<li>Fine-tuning of the self-supervised model
<ul class="org-ul">
<li>This led to somewhat improved results, but doesn't seem to be worth the extra work and longer training time</li>
</ul></li>
</ol></li>
<li>No direct tuning of network or hyper-parameters done: They copied a previous ResNet50 architecture used for self-supervised training.</li>
<li>The hand-curated list of strong-lens candidates they use for training (70%) and validation (30%), i.e. their labeled dataset, consist of just 1'615 objects</li>
</ul>
</div>
</div>
<div id="outline-container-orgabdb2e1" class="outline-2">
<h2 id="orgabdb2e1"><span class="section-number-2">5.</span> Why do they do it?</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Manual inspection is no longer feasible&#x2014;even with citizen science approached the rate of data-production outstrips the rate of data-analysis by three orders of magnitude of more.</li>
<li>Supervised CNNs can handle the data volume, but the training labels are biased towards bright and large objects and generally skews towards more common galaxy morphologies.</li>
<li>Supervised CNN struggles when the number of labels in any one class is very small.</li>
<li>A method is needed that is robust to noise (errors) and bias both in the labels.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc2c51af" class="outline-2">
<h2 id="orgc2c51af"><span class="section-number-2">6.</span> Conclusions and open questions</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>"&#x2026;perhaps this is the most important result of this work: the self-supervised model was trained in a generalized task-agnostic way requiring no labelled samples, yet the resulting representations enabled rapid and easy discovery of speciﬁc and rare objects."</li>
<li>The total number of object (sources), and hence cropped images, they train on is about 76 million&#x2014;these are all the sources (bright spots) in several published catalogs, where the source is <i>not</i> classified as a star, because its shape deviated from that expected from the point-spread-function of the telescope.
<ul class="org-ul">
<li>This is comparable to the number of cells in an image-based high-content-screen, with say 100 cells per image, nine images per well, 384 wells per plate, and 200 plates (69 million cells).</li>
</ul></li>
<li>Could this approach, and perhaps even the architecture, be used to identity rare events in an image-based high-content screen?
<ul class="org-ul">
<li>I see no reason why not, as long as the augmentations used to learn the invariants are done intelligently</li>
</ul></li>
<li>The approach looks very general and it is an attractive fact that the computationally heavy part, the training of the CNN, is done only once and in a largely task-agnostic manner
<ul class="org-ul">
<li>Asking new questions of the data-set, i.e. looking for new rare events (say, apoptosis, double-nucleated cells, foci-clusters in the cytoplasm, out-of focus cells, etc. ), can be done in seconds via a similarity search or minutes by training a linear classifier.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Simon F. Nørrelykke</p>
<p class="date">Created: 2021-10-06 Wed 14:51</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
